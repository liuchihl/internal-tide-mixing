#!/bin/bash
#SBATCH --job-name="tilt-2D-CG" 
#SBATCH --output="output_message/2D-idealized_tilt.%j.%N.out"
#SBATCH --partition=gpuA100x4 ##gpuH200x8
#SBATCH --mem=36G
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1 # could be 1 for py-torch
#SBATCH --cpus-per-task=4   # spread out to use 1 core per numa, set to 64 if tasks is 1
#SBATCH --constraint="scratch"
#SBATCH --gpus-per-node=1
#SBATCH --gpu-bind=closest   # select a cpu close to gpu on pci bus topology
#SBATCH --account=bcpi-delta-gpu    # <- match to a "Project" returned by the "accounts" command
#SBATCH --mail-user=chihlul1@uci.edu
#SBATCH --mail-type="BEGIN,END" 
##SBATCH --exclusive  # dedicated node for this job
##SBATCH --no-requeue
#SBATCH -t 48:00:00

module restore internal-tide
export PATH=$HOME/software/julia/current/bin:$PATH
date
rundir1=/scratch/bcpi/cliu28/internal-tide-mixing/run/internal_tide_2Drun_0.jl
rundir2=/scratch/bcpi/cliu28/internal-tide-mixing/run/internal_tide_2Drun_2e-3.jl
rundir3=/scratch/bcpi/cliu28/internal-tide-mixing/run/internal_tide_2Drun_4e-3.jl
rundir4=/scratch/bcpi/cliu28/internal-tide-mixing/run/internal_tide_2Drun_8e-3.jl

# Run fix_cuda.jl first
echo "Running CUDA fix script..."
julia --project fix_cuda.jl
echo "Launching parallel Julia jobs on same GPU..."

CUDA_VISIBLE_DEVICES=0 julia --project $rundir1 > run1.log 2>&1 &
CUDA_VISIBLE_DEVICES=0 julia --project $rundir2 > run2.log 2>&1 &
CUDA_VISIBLE_DEVICES=0 julia --project $rundir3 > run3.log 2>&1 &
CUDA_VISIBLE_DEVICES=0 julia --project $rundir4 > run4.log 2>&1 &

wait
echo "All runs completed."
date
